## ---------------------------------------------------------------------------- DATA-SOURCE CONFIG
spring.datasource.url=jdbc:postgresql://${AB2D_DB_HOST}:${AB2D_DB_PORT}/${AB2D_DB_DATABASE}?sslmode=${AB2D_DB_SSL_MODE:allow}&reWriteBatchedInserts=true
spring.datasource.username=${AB2D_DB_USER}
spring.datasource.password=${AB2D_DB_PASSWORD}
spring.datasource.driver-class-name=org.postgresql.Driver
spring.datasource.hikari.maximumPoolSize=100

## --------------------- @SCHEDULED annotation thread pool size
spring.task.scheduling.pool.size=8

## ---------------------------------------------------------------------------- JPA CONFIG
spring.jpa.hibernate.ddl-auto=validate
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false

## ---------------------------------------------------------------------------- SPRING INTEGRATION CONFIG
spring.integration.jdbc.initialize-schema=always
spring.liquibase.contexts=none

# Seconds between checking for new jobs to run
eob.job.queueing.frequency=10
# Maximum number of patients (per job) whose results need to be processed
# Prevents memory pressure
eob.job.patient.queue.max.size=10000
# Number of patients to load into queue at one time
eob.job.patient.queue.page.size=1000
# Number of EOBs to search per file
eob.job.patient.number.per.thread=${SEARCH_BENE_BATCH_SIZE:100}

## ----------------------

# Amount of time to wait to write out claims to a file before throwing an error
file.try.lock.timeout=30

# How often to write out % complete to the database (for display in status call to API)
report.progress.db.frequency=100

# How often to write out % complete to stdout in milliseconds
report.progress.log.frequency=10000

## fail the job if >= 1% of the records fail
failure.threshold=1

## ---------------------------------------------------------------------------- JOB PROCESSOR THREAD-POOL CONFIG

## These properties apply to "mainJobProcessingPool".
job.core.pool.size=${AB2D_JOB_POOL_CORE_SIZE:#{5}}
job.max.pool.size=${AB2D_JOB_POOL_MAX_SIZE:#{10}}
job.queue.capacity=${AB2D_JOB_QUEUE_CAPACITY:#{0}}

## These properties apply to "patientCoverageThreadPool"
coverage.core.pool.size=10

# Coverage processor parameterization to find stale jobs, stuck jobs, and limit allowed failures
# Run every minute for testing
coverage.update.schedule=0 0/1 * * * ?
# Default until BFD issues are sorted out
coverage.update.max.attempts=3
coverage.update.initial.delay=1
coverage.update.monitoring.interval=0 0/1 * * * ?
coverage.update.load.schedule=0 0/1 * * * ?

# Coverage verification
coverage.verify.schedule=0 0 */12 ? * SUN,MON,WED,THU,FRI,SAT *

## ---------------------------------------------------------------------------- STUCK JOB
## -- run every hour
stuck.job.cron.schedule=0 0 * * * ?
stuck.job.cancel.threshold=36

## ---------------------------------------------------------------------------- ROLLOVER IN MB FOR OUTPUT FILES
job.file.rollover.ndjson=200

## ---------------------------------------------------------------------------- LOGGING LEVEL
logging.level.root=INFO
logging.level.gov.cms.ab2d=INFO

logging.level.org.springframework=WARN
logging.level.com.zaxxer.hikari=WARN
logging.level.org.hibernate=WARN
logging.level.liquibase=WARN

health.requiredSpareMemoryInMB=32
health.urlsToCheck=http://www.google.com,http://www.facebook.com

spring.autoconfigure.exclude[0]=org.springframework.boot.autoconfigure.security.servlet.SecurityAutoConfiguration

## ---------------------------------------------------------------------------- BFD Health Check
bfd.health.check.schedule=*/15 * * * * ?
bfd.health.check.consecutive.successes=5
bfd.health.check.consecutive.failures=5

## ---------------------------------------------------------------------------- Earliest Data Date
bfd.earliest.data.date=${AB2D_EARLIEST_DATA_DATE:#{'01/01/2020'}}

cloud.aws.stack.auto=false
## ---------------------------------------------------------------------------- Aggregator
aggregator.multiplier=3
aggregator.directory.finished=finished
aggregator.directory.streaming=streaming
aggregator.file.buffer.size=1048576
