require 'aws-sdk'
require 'pg'

class SecureS3Objects

  def encrypt_and_put_file_into_s3(source_file, s3_bucket, s3_folder, region_name, kms_key_id)
    
    file_name = File.basename source_file
    
    kms = Aws::KMS::Client.new(region: region_name)
    s3 = Aws::S3::Encryption::Client.new(
      region: region_name,
      kms_key_id: kms_key_id,
      kms_client: kms,
    )
    s3_object_key = "#{s3_folder}/#{file_name}.encrypted"

    File.open(source_file, "rb") do |file|
      resp = s3.put_object(bucket: s3_bucket, key: s3_object_key, body: file)
    end
    
  end

  def get_file_from_s3_and_decrypt(source_file, s3_bucket, s3_folder, region_name, kms_key_id, destination_dir)

    file_name = File.basename source_file
    
    kms = Aws::KMS::Client.new(region: region_name)
    s3 = Aws::S3::Encryption::Client.new(
      region: region_name,
      kms_key_id: kms_key_id,
      kms_client: kms
    )
    s3_object_key = "#{s3_folder}/#{file_name}.encrypted"
    
    path_components = s3_object_key.split('/')
    encrypted_file_name = path_components[path_components.length - 1]
    file_name = encrypted_file_name.sub('.encrypted', '')
    path_and_file_name = "#{destination_dir}/#{file_name}"

    File.open(path_and_file_name, "wb") do |file|
      resp = s3.get_object({ bucket: s3_bucket, key: s3_object_key, response_target: path_and_file_name})
    end

  end
  
end

desc 'Encrypt and put file into S3'
task :encrypt_and_put_file_into_s3, [:param1, :param2] do |t, args|

  if "#{args.param2}" == "ab2d-east-prod-test-main"
    kms_key_id = "alias/ab2d-east-prod-test-main-kms"
  else
    kms_key_id = "alias/ab2d-kms"
  end

  source_path_and_file = "#{args.param1}"
  s3_bucket = "#{args.param2}"
  s3 = SecureS3Objects.new
  s3.encrypt_and_put_file_into_s3("#{source_path_and_file}", "#{s3_bucket}", 'encrypted-files', 'us-east-1', "#{kms_key_id}")
end

desc 'Get file from S3 and decrypt'
task :get_file_from_s3_and_decrypt, [:param1, :param2] do |t, args|

  if "#{args.param2}" == "ab2d-east-prod-test-main"
    kms_key_id = "alias/ab2d-east-prod-test-main-kms"
  else
    kms_key_id = "alias/ab2d-kms"
  end

  source_path_and_file = "#{args.param1}"
  s3_bucket = "#{args.param2}"
  s3 = SecureS3Objects.new
  s3.get_file_from_s3_and_decrypt("#{source_path_and_file}", "#{s3_bucket}", 'encrypted-files', 'us-east-1', "#{kms_key_id}", '/tmp')
end

desc 'Get schema as a sql script and data as CSV files'
task :database_get_data do

  # Get environment variables

  home = `printf ${HOME}`
  cms_env = `printf $SOURCE_CMS_ENV`
  database_user = `printf $DATABASE_USER`
  PGPASSWORD = `printf $PGPASSWORD`
  database_name = `printf $DATABASE_NAME`
  database_schema_name = `printf $DATABASE_SCHEMA_NAME`
  database_host = `printf $DATABASE_HOST`
  database_port = `printf $DATABASE_PORT`

  # Generate CSV files from database tables

  Dir.chdir("#{home}/database_backup/#{cms_env}/csv") do

    # Delete existing CSV files and any CSV split results that may still exist

    sh "rm -f *.csv*"

    # Query information schema of source database for list of tables

    psql_conn = PG.connect(:dbname => "#{database_name}", :host => "#{database_host}", :port => "#{database_port}", :user => "#{database_user}")
    tables = psql_conn.exec("SELECT table_name FROM information_schema.tables WHERE table_schema = '#{database_schema_name}' AND table_type = 'BASE TABLE' ORDER BY table_name;")
    psql_conn.close()

    # Loop through the list of tables and run this for each table

    for table in tables
      table_name = table['table_name']
      sh "psql --dbname=#{database_name} --host=#{database_host} --username=#{database_user} --command='\\COPY (SELECT * FROM #{database_schema_name}.\"#{table_name}\") TO '#{database_schema_name}.#{table_name}.csv' WITH (FORMAT CSV);'"
    end

    # Loop through generated CSV files and split any files that are greater than 25 MB

    Dir['./*.csv'].sort.each do |csv_file|

      csv_file_name = "#{csv_file}".split('/')[1]
      table_name = "#{csv_file_name}".split('.')[1]
      file_size_test = ('%.2i' % (File.size("#{csv_file_name}").to_i / 2**20 / 25)).to_i

      if file_size_test > 0

        # Split file by the "file_size_test" value

        file_number_of_lines = `wc -l #{csv_file_name}`.split(' ')[0]

        # Get max number of line per file

        max_number_of_lines = file_number_of_lines.to_i / file_size_test + 1

        # Split up the file into multiple files

        `split -l #{max_number_of_lines} #{csv_file_name} #{csv_file_name}@`

        # Delete the original file

        `rm -f #{csv_file_name}`

        # Get the part count

        part_count = 0
        Dir["./#{csv_file_name}*"].sort.each do |csv_file_part|
          part_count = part_count + 1
        end

        # Rename file parts

        part_num = 0
        Dir["./#{csv_file_name}@*"].sort.each do |csv_file_part|
          part_num = part_num + 1
          if (part_num < 10)
            part_num_string = "0#{part_num}"
          else
            part_num_string = "#{part_num}"
          end
          `mv #{csv_file_part} #{database_schema_name}.#{table_name}_part#{part_num_string}.csv`
        end

      end

    end

  end

  puts ""
  puts "**********************************************************************************"
  puts "CSV files for the '#{cms_env}' environment can be found here"
  puts "**********************************************************************************"
  puts ""
  puts "-------------"
  puts "AWS account"
  puts "-------------"
  puts "management"
  puts ""
  puts "-------------"
  puts "EC2 instance"
  puts "-------------"
  puts "Jenkins agent"
  puts ""
  puts "-------------"
  puts "Directory"
  puts "-------------"
  puts "#{home}/database_backup/#{cms_env}/csv"
  puts ""
  puts "**********************************************************************************"

end
